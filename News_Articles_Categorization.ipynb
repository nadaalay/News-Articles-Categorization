{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Articles Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    " According to Internet Live Stats, in every second, around 9,000 tweets are posted; 3 million emails are sent and more than 90,000 GB of Internet traffic. So, we have a big amount of data in the internet. With such data, it is necessary to classify the data in categories. News contents are one type of data that should be categorized into groups to be easily accessed. News categorization helps users to access news of their interest without wasting time.\n",
    " \n",
    "In this project,  I will apply machine learning algorithms on news collected from CNN News website to construct a model that classify the news into groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "The dataset of this project from [kaggle](www.kaggle.com/c/learn-ai-bbc/data).\n",
    "\n",
    "* BBC News Train.csv - The file contains a dataset of 1490 BBC News articles that classified into five categories: Business, Tech, Politics, Sport, Entertainment.\n",
    "\n",
    "**Data Fields:**\n",
    "\n",
    "   - ArticleId - Article id unique # given to the record.\n",
    "   - Article - text of the header and article.\n",
    "   - Category - cateogry of the article (tech, business, sport, entertainment, politics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The steps to build the model are:**\n",
    "1. Data Exploration\n",
    "2. Text Processing\n",
    "3. Feature Extraction\n",
    "4. Modeling\n",
    "5. Use the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost \n",
    "from sklearn.metrics  import classification_report\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset from csv file\n",
    "df1 = pd.read_csv('BBC News Training.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'tech', 'politics', 'sport', 'entertainment']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all category\n",
    "category = list(df1['Category'].unique())\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1['Category'].value_counts().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleId    0\n",
       "Text         0\n",
       "Category     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def preprocess(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function: split text into words and return the root form of the words\n",
    "    Args:\n",
    "      text(str): the article\n",
    "    Return:\n",
    "      lem(list of str): a list of the root form of the article words\n",
    "    \"\"\"\n",
    "        \n",
    "    # Normalize text\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", str(text).lower())\n",
    "    \n",
    "    # Tokenize text\n",
    "    token = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop = stopwords.words(\"english\")\n",
    "    words = [t for t in token if t not in stop]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lem = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    \n",
    "    return lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Preprocessed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "      <td>[worldcom, ex, bos, launch, defence, lawyer, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "      <td>[german, business, confidence, slide, german, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "      <td>[bbc, poll, indicates, economic, gloom, citize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[lifestyle, governs, mobile, choice, faster, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "      <td>[enron, boss, payout, eighteen, former, enron,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1582</td>\n",
       "      <td>howard  truanted to play snooker  conservative...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[howard, truanted, play, snooker, conservative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>651</td>\n",
       "      <td>wales silent on grand slam talk rhys williams ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[wale, silent, grand, slam, talk, rhys, willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1797</td>\n",
       "      <td>french honour for director parker british film...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[french, honour, director, parker, british, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2034</td>\n",
       "      <td>car giant hit by mercedes slump a slump in pro...</td>\n",
       "      <td>business</td>\n",
       "      <td>[car, giant, hit, mercedes, slump, slump, prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1866</td>\n",
       "      <td>fockers fuel festive film chart comedy meet th...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[fockers, fuel, festive, film, chart, comedy, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  \\\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...   \n",
       "1        154  german business confidence slides german busin...   \n",
       "2       1101  bbc poll indicates economic gloom citizens in ...   \n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...   \n",
       "4        917  enron bosses in $168m payout eighteen former e...   \n",
       "5       1582  howard  truanted to play snooker  conservative...   \n",
       "6        651  wales silent on grand slam talk rhys williams ...   \n",
       "7       1797  french honour for director parker british film...   \n",
       "8       2034  car giant hit by mercedes slump a slump in pro...   \n",
       "9       1866  fockers fuel festive film chart comedy meet th...   \n",
       "\n",
       "        Category                                  Preprocessed_Text  \n",
       "0       business  [worldcom, ex, bos, launch, defence, lawyer, d...  \n",
       "1       business  [german, business, confidence, slide, german, ...  \n",
       "2       business  [bbc, poll, indicates, economic, gloom, citize...  \n",
       "3           tech  [lifestyle, governs, mobile, choice, faster, b...  \n",
       "4       business  [enron, boss, payout, eighteen, former, enron,...  \n",
       "5       politics  [howard, truanted, play, snooker, conservative...  \n",
       "6          sport  [wale, silent, grand, slam, talk, rhys, willia...  \n",
       "7  entertainment  [french, honour, director, parker, british, fi...  \n",
       "8       business  [car, giant, hit, mercedes, slump, slump, prof...  \n",
       "9  entertainment  [fockers, fuel, festive, film, chart, comedy, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"Preprocessed_Text\"] = df1['Text'].apply(lambda x: preprocess(x))\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Text Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common words in each category\n",
    "def find_common_words(df, category):\n",
    "        \n",
    "    \"\"\"\n",
    "    Function: find the most frequent words in the category and return the them\n",
    "    Args:\n",
    "      df(dataframe): the dataframe of articles\n",
    "      category(str): the category name\n",
    "    Return:\n",
    "      the most frequant words in the category\n",
    "    \"\"\"\n",
    "        \n",
    "    # Create dataframes for the category\n",
    "    cat_df = df[df[\"Category\"]==category]\n",
    "    \n",
    "    # Initialize words list for the category\n",
    "    words = [word for tokens in cat_df[\"Preprocessed_Text\"] for word in tokens]\n",
    "    \n",
    "    # Count words frequency\n",
    "    words_counter = Counter(words)\n",
    " \n",
    "    return words_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in each category\n",
      "business  News\n",
      "[('said', 1100), ('year', 618), ('bn', 535), ('u', 523), ('mr', 394), ('company', 393), ('firm', 374), ('market', 346), ('would', 309), ('also', 279)]\n",
      "\n",
      "tech  News\n",
      "[('said', 1064), ('people', 647), ('mobile', 437), ('phone', 396), ('game', 396), ('technology', 380), ('service', 371), ('one', 367), ('year', 364), ('mr', 350)]\n",
      "\n",
      "politics  News\n",
      "[('said', 1445), ('mr', 1100), ('would', 712), ('labour', 494), ('election', 479), ('government', 471), ('party', 464), ('blair', 396), ('minister', 373), ('people', 372)]\n",
      "\n",
      "sport  News\n",
      "[('said', 636), ('game', 487), ('year', 448), ('first', 350), ('win', 337), ('time', 336), ('england', 329), ('player', 311), ('two', 290), ('back', 279)]\n",
      "\n",
      "entertainment  News\n",
      "[('film', 721), ('said', 594), ('year', 450), ('best', 430), ('award', 336), ('u', 290), ('also', 277), ('one', 274), ('show', 273), ('star', 259)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Most common words in each category\")\n",
    "for c in category:\n",
    "    print(c, \" News\")\n",
    "    print(find_common_words(df1, c))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Preprocessed_Text</th>\n",
       "      <th>Preprocessed_Text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "      <td>[worldcom, ex, bos, launch, defence, lawyer, d...</td>\n",
       "      <td>worldcom ex bos launch defence lawyer defendin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "      <td>[german, business, confidence, slide, german, ...</td>\n",
       "      <td>german business confidence slide german busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "      <td>[bbc, poll, indicates, economic, gloom, citize...</td>\n",
       "      <td>bbc poll indicates economic gloom citizen majo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[lifestyle, governs, mobile, choice, faster, b...</td>\n",
       "      <td>lifestyle governs mobile choice faster better ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "      <td>[enron, boss, payout, eighteen, former, enron,...</td>\n",
       "      <td>enron boss payout eighteen former enron direct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category  \\\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business   \n",
       "1        154  german business confidence slides german busin...  business   \n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business   \n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech   \n",
       "4        917  enron bosses in $168m payout eighteen former e...  business   \n",
       "\n",
       "                                   Preprocessed_Text  \\\n",
       "0  [worldcom, ex, bos, launch, defence, lawyer, d...   \n",
       "1  [german, business, confidence, slide, german, ...   \n",
       "2  [bbc, poll, indicates, economic, gloom, citize...   \n",
       "3  [lifestyle, governs, mobile, choice, faster, b...   \n",
       "4  [enron, boss, payout, eighteen, former, enron,...   \n",
       "\n",
       "                                  Preprocessed_Text2  \n",
       "0  worldcom ex bos launch defence lawyer defendin...  \n",
       "1  german business confidence slide german busine...  \n",
       "2  bbc poll indicates economic gloom citizen majo...  \n",
       "3  lifestyle governs mobile choice faster better ...  \n",
       "4  enron boss payout eighteen former enron direct...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Preprocessed_Text2'] = df1['Preprocessed_Text'].apply(' '.join)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine data and target\n",
    "X = df1['Preprocessed_Text2']\n",
    "y = df1['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use TF-IDF method to extract the text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF\n",
    "tf_vec = TfidfVectorizer()\n",
    "train_features = tf_vec.fit(X_train)\n",
    "train_features = tf_vec.transform(X_train)\n",
    "test_features = tf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will use the below machine learning algorithms then I will select the best one based on its training time, accuracy score and f-beta scores. \n",
    "\n",
    "* Support Vector Machine \n",
    "* Ada Boost\n",
    "* Gradient Boosting\n",
    "* XG Boost\n",
    "* Decision Tree\n",
    "* Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate model\n",
    "def fit_eval_model(model, train_features, y_train, test_features, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function: train and evaluate a machine learning classifier.\n",
    "    Args:\n",
    "      model: machine learning classifier\n",
    "      train_features: train data extracted features\n",
    "      y_train: train data lables\n",
    "      test_features: train data extracted features\n",
    "      y_test: train data lables\n",
    "    Return:\n",
    "      results(dictionary): a dictionary of the model training time and classification report\n",
    "    \"\"\"\n",
    "    results ={}\n",
    "    \n",
    "    # Start time\n",
    "    start = time.time()\n",
    "    # Train the model\n",
    "    model.fit(train_features, y_train)\n",
    "    # End time\n",
    "    end = time.time()\n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "    \n",
    "    # Test the model\n",
    "    train_predicted = model.predict(train_features)\n",
    "    test_predicted = model.predict(test_features)\n",
    "    \n",
    "     # Classification report\n",
    "    results['classification_report'] = classification_report(y_test, test_predicted)\n",
    "        \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "sv = svm.SVC()\n",
    "ab = AdaBoostClassifier(random_state = 1)\n",
    "gb = GradientBoostingClassifier(random_state = 1)\n",
    "xgb = xgboost.XGBClassifier(random_state = 1)\n",
    "tree = DecisionTreeClassifier()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "\n",
    "# Fit and evaluate models\n",
    "results = {}\n",
    "for cls in [sv, ab, gb, xgb, tree, nb]:\n",
    "    cls_name = cls.__class__.__name__\n",
    "    results[cls_name] = {}\n",
    "    results[cls_name] = fit_eval_model(cls, train_features, y_train, test_features, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "\n",
      "train_time :\n",
      "6.316733121871948\n",
      "\n",
      "classification_report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.97      0.97      0.97        65\n",
      "entertainment       0.98      0.98      0.98        61\n",
      "     politics       0.98      0.98      0.98        51\n",
      "        sport       0.97      1.00      0.99        70\n",
      "         tech       0.98      0.94      0.96        51\n",
      "\n",
      "     accuracy                           0.98       298\n",
      "    macro avg       0.98      0.97      0.98       298\n",
      " weighted avg       0.98      0.98      0.98       298\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "AdaBoostClassifier\n",
      "\n",
      "train_time :\n",
      "2.5002388954162598\n",
      "\n",
      "classification_report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.56      0.66      0.61        65\n",
      "entertainment       0.85      0.77      0.81        61\n",
      "     politics       0.72      0.71      0.71        51\n",
      "        sport       0.78      0.90      0.83        70\n",
      "         tech       0.89      0.61      0.72        51\n",
      "\n",
      "     accuracy                           0.74       298\n",
      "    macro avg       0.76      0.73      0.74       298\n",
      " weighted avg       0.75      0.74      0.74       298\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "GradientBoostingClassifier\n",
      "\n",
      "train_time :\n",
      "47.05621910095215\n",
      "\n",
      "classification_report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.94      0.94        65\n",
      "entertainment       0.93      0.93      0.93        61\n",
      "     politics       0.96      0.92      0.94        51\n",
      "        sport       0.99      1.00      0.99        70\n",
      "         tech       0.92      0.94      0.93        51\n",
      "\n",
      "     accuracy                           0.95       298\n",
      "    macro avg       0.95      0.95      0.95       298\n",
      " weighted avg       0.95      0.95      0.95       298\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "XGBClassifier\n",
      "\n",
      "train_time :\n",
      "6.50666356086731\n",
      "\n",
      "classification_report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.95      0.97      0.96        65\n",
      "entertainment       0.98      0.98      0.98        61\n",
      "     politics       0.98      0.92      0.95        51\n",
      "        sport       0.97      1.00      0.99        70\n",
      "         tech       0.92      0.92      0.92        51\n",
      "\n",
      "     accuracy                           0.96       298\n",
      "    macro avg       0.96      0.96      0.96       298\n",
      " weighted avg       0.96      0.96      0.96       298\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "DecisionTreeClassifier\n",
      "\n",
      "train_time :\n",
      "0.5982866287231445\n",
      "\n",
      "classification_report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.82      0.82      0.82        65\n",
      "entertainment       0.68      0.75      0.71        61\n",
      "     politics       0.72      0.76      0.74        51\n",
      "        sport       0.82      0.80      0.81        70\n",
      "         tech       0.86      0.73      0.79        51\n",
      "\n",
      "     accuracy                           0.78       298\n",
      "    macro avg       0.78      0.77      0.77       298\n",
      " weighted avg       0.78      0.78      0.78       298\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "MultinomialNB\n",
      "\n",
      "train_time :\n",
      "0.01964402198791504\n",
      "\n",
      "classification_report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.98      0.97        65\n",
      "entertainment       0.98      0.92      0.95        61\n",
      "     politics       0.98      0.98      0.98        51\n",
      "        sport       0.96      1.00      0.98        70\n",
      "         tech       0.96      0.94      0.95        51\n",
      "\n",
      "     accuracy                           0.97       298\n",
      "    macro avg       0.97      0.96      0.97       298\n",
      " weighted avg       0.97      0.97      0.97       298\n",
      "\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classifiers results\n",
    "for res in results:\n",
    "    print (res)\n",
    "    print()\n",
    "    for i in results[res]:\n",
    "        print (i, ':')\n",
    "        print(results[res][i])\n",
    "        print()\n",
    "    print ('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** From the testing results, we can see that **MultinomialNB** classifier is the fastest and most accurate classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Use the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will use the built MultinomialNB model to classify new articles. The articles files inside *Articles folder* and here are the articles sources.  \n",
    "\n",
    "* art1: https://edition.cnn.com/2019/09/30/sport/irish-national-stud-winning-post-spt-intl/index.html\n",
    "* art2: https://edition.cnn.com/2020/04/15/tech/amazon-france-suspension/index.html\n",
    "* art3: https://edition.cnn.com/2020/04/15/politics/barack-obama-2020-test/index.html\n",
    "* art4: https://edition.cnn.com/2020/04/15/entertainment/disney-the-mandalorian-documentary/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify an article\n",
    "def classify_article(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function: classify an article.\n",
    "    Args:\n",
    "      path: the path of the article \n",
    "    Return:\n",
    "      category (str): the category of the article\n",
    "    \"\"\"\n",
    "    # Read file\n",
    "    file = open(path, 'r')\n",
    "    artcl = file.read()\n",
    "\n",
    "    # Text preprocessing\n",
    "    artcl = preprocess(artcl)\n",
    "    artcl = ' '.join(artcl)\n",
    "\n",
    "    # Use TF_IDF\n",
    "    test = tf_vec.transform([artcl])\n",
    "\n",
    "    # Use MultinomialNB model to classify the article\n",
    "    predict = nb.predict(test)\n",
    "    category = predict[0]\n",
    "\n",
    "    # Close file\n",
    "    file.close()\n",
    "\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport\n"
     ]
    }
   ],
   "source": [
    "print(classify_article('Articles/art1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business\n"
     ]
    }
   ],
   "source": [
    "print(classify_article('Articles/art2.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics\n"
     ]
    }
   ],
   "source": [
    "print(classify_article('Articles/art3.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment\n"
     ]
    }
   ],
   "source": [
    "print(classify_article('Articles/art4.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** All articles classified correctly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I used CNN news dataset to classify the news text. I used TF-IDF feature extraction algorithm and six different machine learning algorithms: Support Vector Machine , Ada Boost, Gradient Boosting, XG Boost, Decision Tree, Multinomial Naive Bayes. Multinomial Naive Bayes was the best model and then I use it to classify new articles in order to check the model accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
